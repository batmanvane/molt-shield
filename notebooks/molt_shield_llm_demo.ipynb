{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Molt-Shield LLM Demo\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/batmanvane/molt-shield/blob/main/notebooks/molt_shield_llm_demo.ipynb)\n\n**Real-world example** - This notebook demonstrates a complete workflow where:\n1. Proprietary simulation data is sanitized\n2. An LLM analyzes the sanitized data\n3. LLM suggestions are rehydrated back to original values\n\nThis uses a simulated LLM response (since free API keys require personal accounts), but shows the exact workflow you'd use with Claude, GPT, or Gemini."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup\n\nFirst, mount this notebook from GitHub or upload the files, then install dependencies.\n\n### Option A: Clone from GitHub (recommended)\n```python\n!git clone https://github.com/batmanvane/molt-shield.git /content/molt-shield\n```\n\n### Option B: Upload files manually\nUpload these files to Colab:\n- `src/` folder\n- `config/` folder\n\n### Option C: Install dependencies only\n```python\n!pip install -q lxml pydantic pyyaml requests\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SETUP: Install dependencies (run this FIRST)\n# ============================================================\n# These packages are NOT pre-installed in Google Colab\n\n!pip install -q lxml pydantic pyyaml requests\n\nprint(\"✓ Dependencies installed: lxml, pydantic, pyyaml, requests\")\n\n# Clone repository\n!git clone https://github.com/batmanvane/molt-shield.git /content/molt-shield\n\nimport sys\nsys.path.insert(0, '/content/molt-shield/src')\n\nimport os\nos.chdir('/content/molt-shield')\n\n# Import Molt-Shield modules\nfrom src.config import load_config, MaskingConfig, ShufflingConfig\nfrom src.gatekeeper import apply_gatekeeper, mask_values, _apply_tag_shadowing, DEFAULT_TAG_MAP\nfrom src.policy_engine import Policy, Rule\nfrom src.vault import Vault\nfrom pathlib import Path\nfrom lxml import etree\nimport json\n\nprint(\"✓ Environment ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Your Proprietary Data\n",
    "\n",
    "Imagine this is a proprietary turbine blade simulation - highly valuable IP that you don't want to leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proprietary turbine blade simulation data\n",
    "PROPRIETARY_XML = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<turbine_simulation>\n",
    "    <metadata>\n",
    "        <project>CFD-2024-TURBINE-ALPHA</project>\n",
    "        <engineer>Dr. Smith</engineer>\n",
    "    </metadata>\n",
    "    <blade id=\"blade_001\">\n",
    "        <material>Inconel718</material>\n",
    "        <stress_mpa>850.5</stress_mpa>\n",
    "        <temperature_celsius>650.0</temperature_celsius>\n",
    "        <efficiency>0.92</efficiency>\n",
    "        <lifespan_hours>25000</lifespan_hours>\n",
    "    </blade>\n",
    "    <blade id=\"blade_002\">\n",
    "        <material>Inconel718</material>\n",
    "        <stress_mpa>920.3</stress_mpa>\n",
    "        <temperature_celsius>680.5</temperature_celsius>\n",
    "        <efficiency>0.89</efficiency>\n",
    "        <lifespan_hours>18000</lifespan_hours>\n",
    "    </blade>\n",
    "    <blade id=\"blade_003\">\n",
    "        <material>Ti6Al4V</material>\n",
    "        <stress_mpa>780.0</stress_mpa>\n",
    "        <temperature_celsius>520.0</temperature_celsius>\n",
    "        <efficiency>0.94</efficiency>\n",
    "        <lifespan_hours>35000</lifespan_hours>\n",
    "    </blade>\n",
    "</turbine_simulation>\"\"\"\n",
    "\n",
    "# Save original\n",
    "with open('turbine_data.xml', 'w') as f:\n",
    "    f.write(PROPRIETARY_XML)\n",
    "\n",
    "print(\"=== YOUR PROPRIETARY DATA (NEVER SHARE THIS!) ===\")\n",
    "print(PROPRIETARY_XML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sanitize with Molt-Shield\n",
    "\n",
    "Now let's sanitize this data so it can be safely shared with an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure the policy\nmasking_config = MaskingConfig()\nshuffling_config = ShufflingConfig(seed=42)\n\n# Parse and mask\nxml_tree = etree.fromstring(PROPRIETARY_XML.encode())\nvault = Vault('turbine_vault.json')\n\n# Step 1: Mask numeric values\nmasked_tree = mask_values(xml_tree, masking_config, vault)\n\n# Step 2: Shadow tag names\n# Extend the default map for turbine-specific tags\nTURBINE_TAG_MAP = {**DEFAULT_TAG_MAP, \n    \"stress_mpa\": \"stress_metric\",\n    \"temperature_celsius\": \"thermal_reading\",\n    \"efficiency\": \"performance_factor\",\n    \"lifespan_hours\": \"operational_duration\",\n    \"material\": \"alloy_type\",\n}\n\n_apply_tag_shadowing(masked_tree, TURBINE_TAG_MAP)\nshadowed_xml = etree.tostring(masked_tree, encoding='unicode')\n\n# Step 3: Shuffle siblings (optional - for demo we skip this)\nsanitized_xml = shadowed_xml\n\n# Save vault for later rehydration\nvault.save()\n\nprint(\"=== SANITIZED DATA (SAFE TO SHARE WITH LLM) ===\")\nprint(sanitized_xml)\nprint(f\"\\nVault saved with {len(vault)} entries\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulated LLM Response\n",
    "\n",
    "This is what an LLM like Claude, GPT, or Gemini would return when given the sanitized data.\n",
    "\n",
    "The LLM sees ONLY the sanitized data and makes suggestions using the masked values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated LLM response (in real usage, you'd call an API)\n",
    "LLM_RESPONSE = \"\"\"\n",
    "Based on the sanitized turbine blade data, here are my optimization suggestions:\n",
    "\n",
    "**Analysis:**\n",
    "- blade_001 has moderate stress (VAL_xxx1) and good efficiency (VAL_xxx2)\n",
    "- blade_002 shows high stress (VAL_xxx3) which is concerning\n",
    "- blade_003 has the best efficiency but moderate thermal readings\n",
    "\n",
    "**Recommendations:**\n",
    "1. For blade_002: Reduce stress_metric below VAL_xxx1 by 15%\n",
    "2. Consider material change for blade_002 to match blade_003 alloy_type\n",
    "3. Target operational_duration > 20000 hours - blade_002 needs improvement\n",
    "\n",
    "**Specific changes suggested:**\n",
    "- stress_mpa: VAL_xxx3 → 750.0 (reduce from current high)\n",
    "- thermal_reading: VAL_xxx4 → 600.0 (lower for longevity)\n",
    "- performance_factor: VAL_xxx5 → 0.91 (optimize)\n",
    "\"\"\"\n",
    "\n",
    "# In JSON format (as you'd get from an LLM API)\n",
    "LLM_SUGGESTIONS = {\n",
    "    \"blade_002\": {\n",
    "        \"stress_metric\": \"VAL_xxx1\",  # LLM uses masked value reference\n",
    "        \"thermal_reading\": \"VAL_xxx4\",\n",
    "        \"performance_factor\": \"VAL_xxx5\",\n",
    "        \"alloy_type\": \"Ti6Al4V\"  # This wasn't masked, LLM can see it\n",
    "    },\n",
    "    \"recommendations\": [\n",
    "        \"reduce stress by 15%\",\n",
    "        \"increase operational_duration above 20000\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== LLM RESPONSE (USING MASKED VALUES) ===\")\n",
    "print(LLM_RESPONSE)\n",
    "print(\"\\n=== LLM SUGGESTIONS (JSON) ===\")\n",
    "print(json.dumps(LLM_SUGGESTIONS, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rehydration\n",
    "\n",
    "Now we map the LLM's suggestions back to original values using the vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the vault\nvault = Vault('turbine_vault.json')\nvault.load()\n\n# Rehydrate the suggestions\nprint(\"=== REHYDRATION ===\\n\")\n\n# Find a masked value from our vault\nsample_masked = list(vault.entries.keys())[0]\nsample_original = vault.restore(sample_masked)\n\nprint(f\"Example mapping:\")\nprint(f\"  LLM sees: {sample_masked}\")\nprint(f\"  Actually means: {sample_original}\")\n\n# Rehydrate the JSON suggestions\nrehydrated_suggestions = vault.rehydrate_dict(LLM_SUGGESTIONS)\n\nprint(\"\\n=== REHYDRATED SUGGESTIONS (BACK TO ORIGINAL VALUES) ===\")\nprint(json.dumps(rehydrated_suggestions, indent=2))\n\n# Rehydrate text response (for demonstration)\nrehydrated_text = vault.rehydrate_xml(LLM_RESPONSE)\nprint(\"\\n=== SAMPLE OF REHYDRATED TEXT ===\")\n# Show just a portion\nimport re\nmatches = re.findall(r'VAL_[a-z0-9]+', rehydrated_text)\nfor m in list(set(matches))[:3]:\n    original = vault.restore(m)\n    print(f\"  {m} → {original}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Workflow\n",
    "\n",
    "Here's the full pipeline in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline using apply_gatekeeper\n",
    "policy = Policy(\n",
    "    version=\"1.0\",\n",
    "    global_masking=True,\n",
    "    rules=[\n",
    "        Rule(tag_pattern=\"stress_mpa\", action=\"mask_value\"),\n",
    "        Rule(tag_pattern=\"temperature_celsius\", action=\"mask_value\"),\n",
    "        Rule(tag_pattern=\"efficiency\", action=\"mask_value\"),\n",
    "        Rule(tag_pattern=\"lifespan_hours\", action=\"mask_value\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "config = load_config('config/default.yaml')\n",
    "sanitized_path, vault_path = apply_gatekeeper(Path('turbine_data.xml'), policy, config)\n",
    "\n",
    "print(\"=== COMPLETE PIPELINE ===\\n\")\n",
    "print(f\"Input: turbine_data.xml\")\n",
    "print(f\"Sanitized output: {sanitized_path}\")\n",
    "print(f\"Vault: {vault_path}\")\n",
    "\n",
    "print(\"\\n--- SANITIZED CONTENT ---\")\n",
    "print(sanitized_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with Real LLM APIs\n",
    "\n",
    "To use with a real LLM, you would:\n",
    "\n",
    "### Option 1: Claude Desktop (MCP)\n",
    "```python\n",
    "# Configure in Claude Desktop settings:\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"molt-shield\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"-m\", \"src.server\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Option 2: OpenAI API\n",
    "```python\n",
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze this sanitized data: {sanitized_xml}\"}\n",
    "    ]\n",
    ")\n",
    "# Then rehydrate the response using vault\n",
    "```\n",
    "\n",
    "### Option 3: Gemini API\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content(f\"Analyze: {sanitized_xml}\")\n",
    "# Then rehydrate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Molt-Shield workflow:\n",
    "\n",
    "```\n",
    "1. YOUR DATA → 2. SANITIZE → 3. LLM → 4. REHYDRATE → 5. ORIGINAL\n",
    "   (private)      (safe to share)   (analyzes)    (maps back)\n",
    "```\n",
    "\n",
    "This ensures your proprietary data never leaves your control in an identifiable form."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}